{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실제값을 Y=4X+6 시뮬레이션하는 데이터 값 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "# y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듬\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 6 +4 * X+ np.random.randn(100,1)\n",
    "\n",
    "# X, y 데이터 셋 scatter plot으로 시각화\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**w0과 w1의 값을 최소화 할 수 있도록 업데이트 수행하는 함수 생성**\n",
    "\n",
    "* 예측 배열 y_pred는 np.dot(X, w1.T) + w0 임\n",
    "100개의 데이터 X(1,2,...,100)이 있다면 예측값은 w0 + X(1)*w1 + X(2)*w1 +..+ X(100)*w1이며, 이는 입력 배열 X와 w1 배열의 내적임.\n",
    "* 새로운 w1과 w0를 update함\n",
    "![](./image01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 과 w0 를 업데이트 할 w1_update, w0_update를 반환. \n",
    "def get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n",
    "    N = len(y)\n",
    "    # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화\n",
    "    w1_update = np.zeros_like(w1)\n",
    "    w0_update = np.zeros_like(w0)\n",
    "    # 예측 배열 계산하고 예측과 실제 값의 차이 계산\n",
    "    y_pred = np.dot(X, w1.T) + w0\n",
    "    diff = y-y_pred\n",
    "         \n",
    "    # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 \n",
    "    w0_factors = np.ones((N,1))\n",
    "\n",
    "    # w1과 w0을 업데이트할 w1_update와 w0_update 계산\n",
    "    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))\n",
    "    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))    \n",
    "    \n",
    "    return w1_update, w0_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.zeros((1,1))\n",
    "w1 = np.zeros((1,1))\n",
    "y_pred = np.dot(X, w1.T) + w0\n",
    "diff = y-y_pred\n",
    "print(diff.shape)\n",
    "w0_factors = np.ones((100,1))\n",
    "w1_update = -(2/100)*0.01*(np.dot(X.T, diff))\n",
    "w0_update = -(2/100)*0.01*(np.dot(w0_factors.T, diff))   \n",
    "print(w1_update.shape, w0_update.shape)\n",
    "w1, w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**반복적으로 경사 하강법을 이용하여 get_weigth_updates()를 호출하여 w1과 w0를 업데이트 하는 함수 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함. \n",
    "def gradient_descent_steps(X, y, iters=10000):\n",
    "    # w0와 w1을 모두 0으로 초기화. \n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "    \n",
    "    # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. \n",
    "    for ind in range(iters):\n",
    "        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n",
    "        w1 = w1 - w1_update\n",
    "        w0 = w0 - w0_update\n",
    "              \n",
    "    return w1, w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예측 오차 비용을 계산을 수행하는 함수 생성 및 경사 하강법 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(y, y_pred):\n",
    "    N = len(y) \n",
    "    cost = np.sum(np.square(y - y_pred))/N\n",
    "    return cost\n",
    "\n",
    "w1, w0 = gradient_descent_steps(X, y, iters=1000)\n",
    "print(\"w1:{0:.3f} w0:{1:.3f}\".format(w1[0,0], w0[0,0]))\n",
    "y_pred = w1[0,0] * X + w0\n",
    "print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**미니 배치 확률적 경사 하강법을 이용한 최적 비용함수 도출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):\n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "    prev_cost = 100000\n",
    "    iter_index =0\n",
    "    \n",
    "    for ind in range(iters):\n",
    "        np.random.seed(ind)\n",
    "        # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장\n",
    "        stochastic_random_index = np.random.permutation(X.shape[0])\n",
    "        sample_X = X[stochastic_random_index[0:batch_size]]\n",
    "        sample_y = y[stochastic_random_index[0:batch_size]]\n",
    "        # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트\n",
    "        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n",
    "        w1 = w1 - w1_update\n",
    "        w0 = w0 - w0_update\n",
    "    \n",
    "    return w1, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)\n",
    "print(\"w1:\",round(w1[0,0],3),\"w0:\",round(w0[0,0],3))\n",
    "y_pred = w1[0,0] * X + w0\n",
    "print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  #사이킷런 1.2 부터는 보스턴 주택가격 데이터가 없어진다는 warning 메시지 출력 제거\n",
    "%matplotlib inline\n",
    "\n",
    "# boston 데이타셋 로드\n",
    "boston = load_boston()\n",
    "\n",
    "# boston 데이타셋 DataFrame 변환 \n",
    "bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)\n",
    "\n",
    "# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. \n",
    "bostonDF['PRICE'] = boston.target\n",
    "print('Boston 데이타셋 크기 :',bostonDF.shape)\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRIM: 지역별 범죄 발생률  \n",
    "* ZN: 25,000평방피트를 초과하는 거주 지역의 비율\n",
    "* NDUS: 비상업 지역 넓이 비율\n",
    "* CHAS: 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0)\n",
    "* NOX: 일산화질소 농도\n",
    "* RM: 거주할 수 있는 방 개수\n",
    "* AGE: 1940년 이전에 건축된 소유 주택의 비율\n",
    "* DIS: 5개 주요 고용센터까지의 가중 거리\n",
    "* RAD: 고속도로 접근 용이도\n",
    "* TAX: 10,000달러당 재산세율\n",
    "* PTRATIO: 지역의 교사와 학생 수 비율\n",
    "* B: 지역의 흑인 거주 비율\n",
    "* LSTAT: 하위 계층의 비율\n",
    "* MEDV: 본인 소유의 주택 가격(중앙값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 컬럼별로 주택가격에 미치는 영향도를 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.\n",
    "fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)\n",
    "lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']\n",
    "for i , feature in enumerate(lm_features):\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현\n",
    "    sns.regplot(x=feature , y='PRICE',data=bostonDF , ax=axs[row][col])\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "fig1.savefig('p322_boston.tif', format='tif', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습과 테스트 데이터 세트로 분리하고 학습/예측/평가 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156)\n",
    "\n",
    "# Linear Regression OLS로 학습/예측/평가 수행. \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train ,y_train )\n",
    "y_preds = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))\n",
    "print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('절편 값:',lr.intercept_)\n",
    "print('회귀 계수값:', np.round(lr.coef_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index가 칼럼명에 유의\n",
    "coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns )\n",
    "coeff.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시  RMSE 구함. \n",
    "neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# cross_val_score(scoring=\"neg_mean_squared_error\")로 반환된 값은 모두 음수 \n",
    "print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))\n",
    "print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2))\n",
    "print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-5. Polynomial Regression과 오버피팅/언더피팅 이해\n",
    "### Polynomial Regression 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures 클래스로 다항식 변환\n",
    "\n",
    "![](./image02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# 다항식으로 변환한 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성\n",
    "X = np.arange(4).reshape(2,2)\n",
    "print('일차 단항식 계수 feature:\\n',X )\n",
    "\n",
    "# degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly.fit(X)\n",
    "poly_ftr = poly.transform(X)\n",
    "print('변환된 2차 다항식 계수 feature:\\n', poly_ftr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차 다항식 결정값을 구하는 함수 polynomial_func(X) 생성. 즉 회귀식은 결정값 y = 1+ 2x_1 + 3x_1^2 + 4x_2^3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
    "    print(X[:, 0])\n",
    "    print(X[:, 1])\n",
    "    return y\n",
    "\n",
    "X = np.arange(0,4).reshape(2,2)\n",
    "\n",
    "print('일차 단항식 계수 feature: \\n' ,X)\n",
    "y = polynomial_func(X)\n",
    "print('삼차 다항식 결정값: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차 다항식 계수의 피처값과 3차 다항식 결정값으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 차 다항식 변환 \n",
    "poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)\n",
    "print('3차 다항식 계수 feature: \\n',poly_ftr)\n",
    "\n",
    "# Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인\n",
    "model = LinearRegression()\n",
    "model.fit(poly_ftr,y)\n",
    "print('Polynomial 회귀 계수\\n' , np.round(model.coef_, 2))\n",
    "print('Polynomial 회귀 Shape :', model.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사이킷런 파이프라인(Pipeline)을 이용하여 3차 다항회귀 학습**  \n",
    "\n",
    "사이킷런의 Pipeline 객체는 Feature 엔지니어링 변환과 모델 학습/예측을 순차적으로 결합해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 \n",
    "    return y\n",
    "\n",
    "# Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                  ('linear', LinearRegression())])\n",
    "X = np.arange(4).reshape(2,2)\n",
    "y = polynomial_func(X)\n",
    "\n",
    "model = model.fit(X, y)\n",
    "print('Polynomial 회귀 계수\\n', np.round(model.named_steps['linear'].coef_, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항 회귀를 이용한 과소적합 및 과적합 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cosine 곡선에 약간의 Noise 변동값을 더하여 실제값 곡선을 만듬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "\n",
    "# 임의의 값으로 구성된 X값에 대해 코사인 변환 값을 반환.\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# X는 0부터 1까지 30개의 임의의 값을 순서대로 샘플링한 데이터입니다.\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "\n",
    "# y 값은 코사인 기반의 true_fun()에서 약간의 노이즈 변동 값을 더한 값입니다.\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "# 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다.\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    \n",
    "    # 개별 degree별로 Polynomial 변환합니다.\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X.reshape(-1, 1), y)\n",
    "    \n",
    "    # 교차 검증으로 다항 회귀를 평가합니다.\n",
    "    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    # Pipeline을 구성하는 세부 객체를 접근하는 named_steps['객체명']을 이용해 회귀계수 추출\n",
    "    coefficients = pipeline.named_steps['linear_regression'].coef_\n",
    "    print('\\nDegree {0} 회귀 계수는 {1} 입니다.'.format(degrees[i], np.round(coefficients, 2)))\n",
    "    print('Degree {0} MSE 는 {1} 입니다.'.format(degrees[i], -1*np.mean(scores)))\n",
    "          \n",
    "    # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다.\n",
    "    # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다.\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    # 예측값 곡선\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    # 실제 값 곡선\n",
    "    plt.plot(X_test, true_fun(X_test), '--', label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(degrees[i], -scores.mean(), scores.std()))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-6. Regularized Linear Models – Ridge, Lasso\n",
    "### Regularized Linear Model - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# boston 데이타셋 로드\n",
    "boston = load_boston()\n",
    "\n",
    "# boston 데이타셋 DataFrame 변환 \n",
    "bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)\n",
    "\n",
    "# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. \n",
    "bostonDF['PRICE'] = boston.target\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha = 10)\n",
    "neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))\n",
    "print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))\n",
    "print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha값을 0 , 0.1 , 1 , 10 , 100 으로 변경하면서 RMSE 측정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 릿지에 사용될 alpha 파라미터의 값을 정의\n",
    "alphas = [0, 0.1, 1, 10, 100]\n",
    "\n",
    "# alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함.\n",
    "for alpha in alphas :\n",
    "    ridge = Ridge(alpha = alpha)\n",
    "    \n",
    "    # cross_val_score를 이용해 5 폴드의 평균 RMSE를 계산\n",
    "    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "    print('alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} '.format(alpha, avg_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**각 alpha에 따른 회귀 계수 값을 시각화. 각 alpha값 별로 plt.subplots로 맷플롯립 축 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성  \n",
    "fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)\n",
    "# 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성  \n",
    "coeff_df = pd.DataFrame()\n",
    "\n",
    "# alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정\n",
    "for pos , alpha in enumerate(alphas) :\n",
    "    ridge = Ridge(alpha = alpha)\n",
    "    ridge.fit(X_data , y_target)\n",
    "    # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가.  \n",
    "    coeff = pd.Series(data=ridge.coef_ , index=X_data.columns )\n",
    "    colname='alpha:'+str(alpha)\n",
    "    coeff_df[colname] = coeff\n",
    "    # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현\n",
    "    coeff = coeff.sort_values(ascending=False)\n",
    "    axs[pos].set_title(colname)\n",
    "    axs[pos].set_xlim(-3,6)\n",
    "    sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])\n",
    "\n",
    "# for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha 값에 따른 컬럼별 회귀계수 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alphas = [0 , 0.1 , 1 , 10 , 100]\n",
    "sort_column = 'alpha:'+str(ridge_alphas[0])\n",
    "coeff_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라쏘 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 \n",
    "def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, \n",
    "                        verbose=True, return_coeff=True):\n",
    "    coeff_df = pd.DataFrame()\n",
    "    if verbose : print('####### ', model_name , '#######')\n",
    "    for param in params:\n",
    "        if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "        elif model_name =='Lasso': model = Lasso(alpha=param)\n",
    "        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "        neg_mse_scores = cross_val_score(model, X_data_n, \n",
    "                                             y_target_n, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))\n",
    "        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출\n",
    "        \n",
    "        model.fit(X_data_n , y_target_n)\n",
    "        if return_coeff:\n",
    "            # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. \n",
    "            coeff = pd.Series(data=model.coef_ , index=X_data_n.columns )\n",
    "            colname='alpha:'+str(param)\n",
    "            coeff_df[colname] = coeff\n",
    "    \n",
    "    return coeff_df\n",
    "# end of get_linear_regre_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]\n",
    "coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력\n",
    "sort_column = 'alpha:'+str(lasso_alphas[0])\n",
    "coeff_lasso_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엘라스틱넷 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "# l1_ratio는 0.7로 고정\n",
    "elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]\n",
    "coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas,\n",
    "                                      X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력\n",
    "sort_column = 'alpha:'+str(elastic_alphas[0])\n",
    "coeff_elastic_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀 모델을 위한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정\n",
    "# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. \n",
    "def get_scaled_data(method='None', p_degree=None, input_data=None):\n",
    "    if method == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif method == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif method == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "\n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, \n",
    "                                         include_bias=False).fit_transform(scaled_data)\n",
    "    \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. \n",
    "alphas = [0.1, 1, 10, 100]\n",
    "#변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성\n",
    "# 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 \n",
    "scale_methods=[(None, None), ('Standard', None), ('Standard', 2), \n",
    "               ('MinMax', None), ('MinMax', 2), ('Log', None)]\n",
    "for scale_method in scale_methods:\n",
    "    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], \n",
    "                                    input_data=X_data)\n",
    "    print(X_data_scaled.shape, X_data.shape)\n",
    "    print('\\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))\n",
    "    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, \n",
    "                        y_target_n=y_target, verbose=False, return_coeff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
